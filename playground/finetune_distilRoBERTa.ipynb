{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldacosta/miniconda3/envs/work/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Finetuning the toxic and nontoxic language models\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments, AdamW, EarlyStoppingCallback\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import argparse\n",
    "import random\n",
    "from IPython import embed\n",
    "from utils import *\n",
    "from training.infilling import text_infill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG Purpose\n",
    "# parser = argparse.ArgumentParser()\n",
    "# args = parser.parse_args()\n",
    "class MyDict:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        for key, value in data.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = MyDict({\n",
    "    \"tok_type\": \"distilroberta-base\",\n",
    "    \"model_type\": \"distilroberta-base\",\n",
    "    \"train_data\": \"dataset/train/train_toxic.csv\",\n",
    "    \"val_data\": \"dataset/train/val_toxic.csv\",\n",
    "    \"model_dir\": \"models/toxic\",\n",
    "    \"max_source_length\": 180,\n",
    "    \"max_target_length\": 230,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"eval_batch_size\": 128,\n",
    "    \"max_steps\": 50000,\n",
    "    \"lr\": 1e-6,\n",
    "    \"logging_steps\": 500,\n",
    "    \"seed\": 0,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"save_steps\": 500,\n",
    "    \"data_type\": \"jigsaw_full_30\",\n",
    "    \"logging_dir\": \"logs\",\n",
    "    \"early_stopping_steps\": 5,\n",
    "    \"load_old\": None\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"No GPUs found!\")\n",
    "else:\n",
    "    print(\"Found\", str(torch.cuda.device_count()), \"GPUS!\")\n",
    "\n",
    "seed_everything(args.seed)\n",
    "\n",
    "# Load in the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(args.tok_type)\n",
    "\n",
    "mask = tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/toxic/distilroberta-base_1e-06_0_0_jigsaw_full_30\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.model_dir):\n",
    "    print(args.model_dir)\n",
    "    os.mkdir(args.model_dir)\n",
    "\n",
    "output_dir = args.model_dir + \"/\" + args.model_type.split(\"/\")[-1] + \"_\" + str(args.lr) + \"_\" + \\\n",
    "str(args.seed) + \"_\" + str(args.train_batch_size * torch.cuda.device_count()) + \"_\" + args.data_type\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic to continue training - look at previous models saved\n",
    "try:\n",
    "    prev_models = os.listdir(output_dir)\n",
    "    # Alpha sort\n",
    "    prev_models.sort()\n",
    "    # Len sort\n",
    "    prev_models.sort(key=len)\n",
    "except:\n",
    "    prev_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic to continue training if we want to load the old model - load pretrained model\n",
    "if args.load_old and len(prev_models) > 0:\n",
    "    model = BartForConditionalGeneration.from_pretrained(os.path.join(output_dir, prev_models[-1]), forced_bos_token_id = tokenizer.bos_token_id).to(device)\n",
    "else:\n",
    "    # Otherwise train a new model\n",
    "    model = RobertaForMaskedLM.from_pretrained(args.model_type, forced_bos_token_id = tokenizer.bos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115216 29118\n"
     ]
    }
   ],
   "source": [
    "train_texts = []\n",
    "val_texts = []\n",
    "\n",
    "# Read/process the data based on which dataset we're using: Jigsaw or Dynabench\n",
    "# If you want to load your own data, put the data loading logic here\n",
    "if \"jigsaw\" in args.data_type:\n",
    "    train = pd.read_csv(args.train_data)\n",
    "    val = pd.read_csv(args.val_data)\n",
    "\n",
    "    train_texts =  train[\"comment_text\"].tolist()\n",
    "    val_texts = val[\"comment_text\"].tolist()\n",
    "\n",
    "print(len(train_texts), len(val_texts))\n",
    "\n",
    "# Reducing dataset for debugging \n",
    "train_texts = [value for i, value in enumerate(train_texts) if i < 5]\n",
    "val_texts = [value for i, value in enumerate(val_texts) if i < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize everything\n",
    "tokenized_labs_train = tokenizer.batch_encode_plus(\n",
    "        train_texts, \n",
    "        max_length = args.max_target_length, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors = \"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_labs_val = tokenizer.batch_encode_plus(\n",
    "    val_texts,\n",
    "    max_length = args.max_target_length, \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors = \"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_labs_val[tokenized_labs_val == tokenizer.pad_token_id] = -100\n",
    "tokenized_labs_train[tokenized_labs_train == tokenizer.pad_token_id] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  8346,     6,  ...,  -100,  -100,  -100],\n",
       "        [    0,  1185,    32,  ...,  -100,  -100,  -100],\n",
       "        [    0,   100,  1017,  ...,  -100,  -100,  -100],\n",
       "        [    0,  2847,  6097,  ...,  -100,  -100,  -100],\n",
       "        [    0, 44950, 46835,  ...,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_labs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this so that examples are dynamically masked\n",
    "class JigsawDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, labels, rawdata):\n",
    "        self.rawdata = rawdata\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Dynamically generate sources\n",
    "        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # item['labels'] = cur_lab\n",
    "\n",
    "        # Do detokenization to ensure that the tokenization matches up later\n",
    "        item = tokenizer.encode_plus(\n",
    "            detokenize(text_infill(self.rawdata[idx], tokenizer.mask_token)),\n",
    "            max_length = args.max_source_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors = \"pt\")\n",
    "        \n",
    "        # Necessary for squeezing purposes\n",
    "        item = {key: torch.tensor(val[0]) for key, val in item.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JigsawDataset(tokenized_labs_train, train_texts)\n",
    "val_dataset = JigsawDataset(tokenized_labs_val, val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize(text_infill(train_texts[0], tokenizer.mask_token))\n",
    "\n",
    "item = tokenizer.encode_plus(\n",
    "            detokenize(text_infill(train_texts[0], tokenizer.mask_token)),\n",
    "            max_length = args.max_source_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(\n",
    "            input_ids = inputs.get(\"input_ids\"), \n",
    "            attention_mask = inputs.get(\"attention_mask\"),\n",
    "            labels = inputs.get(\"labels\")\n",
    "            )\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def evaluate(self, **kwargs):\n",
    "        metrics = super().evaluate(**kwargs)\n",
    "        # Example to see what the model outputs for mask token (see if it outputs something biased)\n",
    "        inputs_masked = [\"Asians are <mask>\"]\n",
    "        print(\"\\n Generations: \", tokenizer.batch_decode(model.generate(tokenizer(inputs_masked, return_tensors = \"pt\").to('cuda').input_ids, do_sample=True, num_return_sequences = 5)),\"\\n\")                                                                                                                                                                        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The current model class (RobertaForMaskedLM) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'RobertaForCausalLM'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/csci662/Project/detoxifying-text-marco/finetune_distilRoBERTa.ipynb Cell 17\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/csci662/Project/detoxifying-text-marco/finetune_distilRoBERTa.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBefore training:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/csci662/Project/detoxifying-text-marco/finetune_distilRoBERTa.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m inputs_masked \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mAsians are <mask>\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/csci662/Project/detoxifying-text-marco/finetune_distilRoBERTa.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Generations: \u001b[39m\u001b[39m\"\u001b[39m, tokenizer\u001b[39m.\u001b[39mbatch_decode(model\u001b[39m.\u001b[39;49mgenerate(tokenizer(inputs_masked, return_tensors \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device)\u001b[39m.\u001b[39;49minput_ids, do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_return_sequences \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m)),\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/work/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/work/lib/python3.10/site-packages/transformers/generation/utils.py:1210\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1207\u001b[0m         synced_gpus \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[39m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[0;32m-> 1210\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_class()\n\u001b[1;32m   1212\u001b[0m \u001b[39m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m generation_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1214\u001b[0m     \u001b[39m# legacy: users may modify the model configuration to control generation -- update the generation config\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[39m# model attribute accordingly, if it was created from the model config\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/work/lib/python3.10/site-packages/transformers/generation/utils.py:1089\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[39mif\u001b[39;00m generate_compatible_classes:\n\u001b[1;32m   1088\u001b[0m     exception_message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Please use one of the following classes instead: \u001b[39m\u001b[39m{\u001b[39;00mgenerate_compatible_classes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1089\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[0;31mTypeError\u001b[0m: The current model class (RobertaForMaskedLM) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'RobertaForCausalLM'}"
     ]
    }
   ],
   "source": [
    "# Print out what the model looks like before we start training\n",
    "print(\"Before training:\")\n",
    "inputs_masked = [\"Asians are <mask>\"]\n",
    "print(\"\\n Generations: \", tokenizer.batch_decode(model.generate(tokenizer(inputs_masked, return_tensors = \"pt\").to(device).input_ids, do_sample=True, num_return_sequences = 5)),\"\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "antiexpert_path = \"CARC/anti-expert-model/bart-base_1e-06_0_32_jigsaw_full_30/checkpoint-40000\"\n",
    "expert_path = \"CARC/expert-model/bart-base_2e-06_0_96_jigsaw_full_30/checkpoint-90000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = BartForConditionalGeneration.from_pretrained(expert_path, forced_bos_token_id = tokenizer.bos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "antiexpert = BartForConditionalGeneration.from_pretrained(antiexpert_path, forced_bos_token_id = tokenizer.bos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "antiexpert.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldacosta/miniconda3/envs/work/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generations:  ['</s><s>Asians are racist.</s>', '</s><s>Asians are stupid.</s>', '</s><s>Asians are stupid.</s>', '</s><s>Asians are racist.</s>', '</s><s>Asians are stupid.</s>'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out what the model looks like before we start training\n",
    "print(\"After training:\")\n",
    "inputs_masked = [\"Asians are <mask>\"]\n",
    "print(\"\\n Generations: \", tokenizer.batch_decode(antiexpert.generate(tokenizer(inputs_masked, return_tensors = \"pt\").to(device).input_ids, do_sample=True, num_return_sequences = 5)),\"\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldacosta/miniconda3/envs/work/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generations:  ['</s><s>Asians are not racist.</s>', '</s><s>Asians are welcome.</s><pad>', '</s><s>Asians are the problem.</s>', '</s><s>Asians are not.</s><pad>', '</s><s>Asians are a minority.</s>'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out what the model looks like before we start training\n",
    "print(\"After training:\")\n",
    "inputs_masked = [\"Asians are <mask>\"]\n",
    "print(\"\\n Generations: \", tokenizer.batch_decode(expert.generate(tokenizer(inputs_masked, return_tensors = \"pt\").to(device).input_ids, do_sample=True, num_return_sequences = 5)),\"\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Infill**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = train_texts[2]\n",
    "mask_token = tokenizer.mask_token\n",
    "lam = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = np.array(nltk.tokenize.casual.casual_tokenize(sentence), dtype = \"object\")\n",
    "masked_idcs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # while (len(masked_idcs) / len(tokenized)) < thresh:\n",
    "span_length = np.random.poisson(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized is ['This' 'story' 'gets' 'more' 'ridiculous' 'by' 'the' 'hour' '!' 'And' ','\n",
      " 'I' 'love' 'that' 'people' 'are' 'sending' 'these' 'guys' 'dildos' 'in'\n",
      " 'the' 'mail' 'now' '.' 'But' 'â€¦' 'if' 'they' 'really' 'think' \"there's\"\n",
      " 'a' 'happy' 'ending' 'in' 'this' 'for' 'any' 'of' 'them' ',' 'I' 'think'\n",
      " \"they're\" 'even' 'more' 'deluded' 'than' 'all' 'of' 'the' 'jokes' 'about'\n",
      " 'them' 'assume' '.']\n",
      "masked idcs are []\n",
      "span length is 2\n"
     ]
    }
   ],
   "source": [
    "# while ((span_length > list_diffs(masked_idcs, len(tokenized))) or \\\n",
    "#     (span_length > max_span(len(masked_idcs), len(tokenized), thresh))):    \n",
    "#     span_length = np.random.poisson(lam = lam)\n",
    "    # print(\"Span length is too long, it is currently:\", span_length)\n",
    "\n",
    "print(\"tokenized is\", tokenized)\n",
    "print(\"masked idcs are\", masked_idcs)\n",
    "print(\"span length is\", span_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final mask ratio: 0.03508771929824561\n"
     ]
    }
   ],
   "source": [
    "if span_length == 0:\n",
    "    start_idx = np.random.randint(0, len(tokenized) + 1)\n",
    "    while ((start_idx in masked_idcs) or (start_idx in (np.array(masked_idcs) + 1))):\n",
    "        print(\"bad, start_idx is\", start_idx)\n",
    "        start_idx = np.random.randint(0, len(tokenized) + 1)\n",
    "    \n",
    "    # print(\"start idx is\", start_idx)\n",
    "    tokenized = np.insert(tokenized, start_idx, mask_token)\n",
    "    bisect.insort(masked_idcs, start_idx)\n",
    "\n",
    "else:\n",
    "    while True:          \n",
    "        start_idx = np.random.randint(0, len(tokenized) - span_length + 1)\n",
    "        idcs = np.arange(start_idx, start_idx + span_length)\n",
    "        \n",
    "        for i in idcs:\n",
    "            if i in masked_idcs or i in (np.array(masked_idcs) + 1):\n",
    "                # print(\"bad i\" , i)\n",
    "                continue\n",
    "        break\n",
    "    \n",
    "    for i in idcs:\n",
    "        bisect.insort(masked_idcs, i)\n",
    "        tokenized[i] = mask_token\n",
    "            #print(\"idcs are\", idcs)\n",
    "    print(\"final mask ratio:\",len(masked_idcs)/len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 6578,     7,    33, 32099,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(inputs_masked, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n",
      "\n",
      " Generations:  ['<pad> Die Asien sind die <unk> maske></s><pad>', '<pad><extra_id_0> s in the west are <unk> mask>.</s>', '<pad><extra_id_0> s</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '<pad></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '<pad><extra_id_0> s have never lived in Canada.</s><pad><pad><pad><pad>'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out what the model looks like before we start training\n",
    "print(\"After training:\")\n",
    "inputs_masked = [\"Asians are <mask>\"]\n",
    "print(\"\\n Generations: \", tokenizer.batch_decode(model.generate(tokenizer(inputs_masked, return_tensors = \"pt\").to(device).input_ids, do_sample=True, num_return_sequences = 5)),\"\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilRoberta Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "\n",
    "# Assuming you've already loaded the model and tokenizer\n",
    "model_name = \"distilroberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = f\"Asians are {tokenizer.mask_token}\" # \"The world will end in <mask>\"\n",
    "\n",
    "input_seq = tokenizer.encode(sequence, return_tensors='pt') # tensor([[0, 133, 232, 40, 253, 11, 50264, 2]])\n",
    "mask_token_index = torch.where(input_seq == tokenizer.mask_token_id)[1] # (tensor([0]), tensor([6])) - we only want the the 2nd dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_logits = model(input_seq).logits\n",
    "masked_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(masked_token_logits, 1, dim=1).indices[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asians are  dying\n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Models from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=1000,\n",
       "evaluation_strategy=steps,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=,\n",
       "fsdp_config=None,\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2.5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=/media/drive2/skyler/rewriting_data/toxic/models/logs,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=50000,\n",
       "metric_for_best_model=eval_loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "optim=adamw_hf,\n",
       "optim_args=None,\n",
       "output_dir=/gscratch/xlab/hallisky/rewriting/src/models/toxic/bart-base_2.5e-05_0_32_jigsaw_randmask,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=128,\n",
       "per_device_train_batch_size=32,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=bart-base_2.5e-05_0_32_jigsaw_randmask,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=/gscratch/xlab/hallisky/rewriting/src/models/toxic/bart-base_2.5e-05_0_32_jigsaw_randmask,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=1000,\n",
       "save_strategy=steps,\n",
       "save_total_limit=2,\n",
       "seed=0,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"anti-expert-args.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=5000,\n",
       "evaluation_strategy=steps,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=,\n",
       "fsdp_config=None,\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2.5e-06,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=/media/drive2/skyler/rewriting_data/toxic/models/logs,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=100000,\n",
       "metric_for_best_model=eval_loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "optim=adamw_hf,\n",
       "optim_args=None,\n",
       "output_dir=/gscratch/xlab/hallisky/rewriting/src/models/nontoxic/bart-base_2.5e-06_0_48_jigsaw_randmask,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=96,\n",
       "per_device_train_batch_size=48,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=bart-base_2.5e-06_0_48_jigsaw_randmask,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=/gscratch/xlab/hallisky/rewriting/src/models/nontoxic/bart-base_2.5e-06_0_48_jigsaw_randmask,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=5000,\n",
       "save_strategy=steps,\n",
       "save_total_limit=2,\n",
       "seed=0,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"expert-args.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
