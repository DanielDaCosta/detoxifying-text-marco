{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "import os\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from IPython import embed\n",
    "from training.infilling import *\n",
    "from utils import preprocess, detokenize, seed_everything\n",
    "import nltk.tokenize.casual\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from rewrite import gen_utils\n",
    "from rewrite import generation_logits_process\n",
    "import pandas as pd\n",
    "import functools\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import html\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG Purpose\n",
    "# parser = argparse.ArgumentParser()\n",
    "# args = parser.parse_args()\n",
    "class MyDict:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        for key, value in data.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = MyDict({\n",
    "    \"thresh\": 1.5,\n",
    "    \"base_path\" : \"facebook/bart-base\", \n",
    "    \"antiexpert_path\" : \"CARC/anti-expert-model/bart-base_1e-06_0_32_jigsaw_full_30/checkpoint-40000\",\n",
    "    \"expert_path\" : \"CARC/expert-model/bart-base_2e-06_0_96_jigsaw_full_30/checkpoint-90000\", \n",
    "    \"tokenizer\" : \"facebook/bart-base\",\n",
    "    \"seed\": 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = args.seed\n",
    "thresh = args.thresh\n",
    "base_path = args.base_path\n",
    "antiexpert_path = args.antiexpert_path\n",
    "expert_path = args.expert_path\n",
    "tokenizer = args.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"No GPUs found!\")\n",
    "else:\n",
    "    print(\"Found\", str(torch.cuda.device_count()), \"GPUS!\")\n",
    "\n",
    "seed = seed\n",
    "seed_everything(seed)\n",
    "\n",
    "# Initalize self.tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(tokenizer)\n",
    "\n",
    "# Initialize models\n",
    "model = BartForConditionalGeneration.from_pretrained(base_path, forced_bos_token_id = tokenizer.bos_token_id).to(device)\n",
    "antiexpert = BartForConditionalGeneration.from_pretrained(antiexpert_path, forced_bos_token_id = tokenizer.bos_token_id).to(device)\n",
    "expert = BartForConditionalGeneration.from_pretrained(expert_path, forced_bos_token_id = tokenizer.bos_token_id).to(device)\n",
    "model.eval()\n",
    "antiexpert.eval()\n",
    "expert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs =  [\"I'm surprised you got it done, seeing as you're all girls!\", \"You are a human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(inputs, return_tensors='pt', padding = True).to(device)\n",
    "cur_labels = [\"KL(base || exp)\",\"KL(base || anti)\",\"JS(exp || anti)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jensen divergence\n",
    "def js_div(a,b, reduction):\n",
    "    return 0.5 * F.kl_div(F.log_softmax(a, dim=-1), F.softmax(b,dim=-1), reduction=reduction) + \\\n",
    "         0.5 * F.kl_div(F.log_softmax(b, dim=-1), F.softmax(a,dim=-1), reduction=reduction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Identifying masks: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "use_base_model_for_divergence = False\n",
    "outputs = []\n",
    "\n",
    "for i in tqdm(range(len(inputs)), desc = \"Identifying masks\"):\n",
    "\n",
    "    # Filter text until pad_token\n",
    "    cur_tok = batch[\"input_ids\"][i]    \n",
    "    pad = torch.where(cur_tok == tokenizer.pad_token_id)[0]\n",
    "    if len(pad) > 0:\n",
    "        pad = pad[0]\n",
    "        cur_tok = cur_tok[:pad]\n",
    "\n",
    "    cur_seq = inputs[i]\n",
    "    tok_map = {}\n",
    "    casual = nltk.tokenize.casual.casual_tokenize(cur_seq)\n",
    "    \n",
    "    tok_map = {}\n",
    "    old_idx = 1\n",
    "    cur_idx = 0\n",
    "    cur_word = casual[0]\n",
    "    for new_idx, c in enumerate(cur_tok):\n",
    "        d = tokenizer.decode(c).strip()\n",
    "        if cur_word.startswith(d):\n",
    "            cur_word = cur_word.replace(d, \"\", 1)\n",
    "            if cur_word == \"\":\n",
    "                tok_map[cur_idx] = list(np.arange(old_idx, new_idx+1))\n",
    "                old_idx = new_idx+1\n",
    "                cur_idx += 1\n",
    "                try:\n",
    "                    cur_word = casual[cur_idx]\n",
    "                except:\n",
    "                    break \n",
    "    #     break\n",
    "    # break\n",
    "    # Default MaRCO implementation: use only the expert and anti-expert and find divergence of prob. distributions on each token in the input\n",
    "    if not use_base_model_for_divergence:\n",
    "        # ignore start and end idxs\n",
    "        ignore_idxs = []\n",
    "\n",
    "        for c_idx, c in enumerate(casual):\n",
    "            punc_only = True\n",
    "            for k in c:\n",
    "                if k not in string.punctuation:\n",
    "                    punc_only = False\n",
    "                    break\n",
    "            if punc_only:\n",
    "                ignore_idxs.append(c_idx)\n",
    "\n",
    "        sum_divs_ea = []\n",
    "        for j in range(len(casual)):\n",
    "            new_seq = casual.copy()\n",
    "            new_seq[j] = tokenizer.mask_token\n",
    "            new_full_seq = detokenize(new_seq)\n",
    "            new_full_seq = re.sub(r\"\\s*<mask>\", \"<mask>\", new_full_seq)\n",
    "\n",
    "            new_tok = tokenizer(new_full_seq,return_tensors=\"pt\").input_ids.to(device)\n",
    "            mask_idx = torch.nonzero(new_tok[0] == tokenizer.mask_token_id)\n",
    "\n",
    "            expert_logits = expert.forward(input_ids = new_tok).logits\n",
    "            antiexpert_logits = antiexpert.forward(input_ids = new_tok).logits\n",
    "            divs_ea = js_div(expert_logits,antiexpert_logits, reduction='none').sum(dim = -1)\n",
    "            all_divs = []\n",
    "            for cor_idx in mask_idx:\n",
    "                all_divs.append(divs_ea[0][cor_idx.item()].item())\n",
    "            sum_divs_ea.append(np.mean(all_divs))\n",
    "\n",
    "        # delete the ignore idxs\n",
    "        mean_norm_ea = np.delete(sum_divs_ea, ignore_idxs)\n",
    "        mean_norm_ea = np.array(mean_norm_ea) / mean_norm_ea.mean()\n",
    "        above_thresh = np.nonzero(mean_norm_ea >= thresh)[0]\n",
    "\n",
    "        new_casual=casual.copy()\n",
    "        for a in above_thresh:\n",
    "            num_below = (np.array(ignore_idxs <= a)).sum()\n",
    "            new_casual[a + num_below] = tokenizer.mask_token\n",
    "\n",
    "        outputs.append(re.sub(r\"\\s*<mask>\", \"<mask>\",detokenize(new_casual)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_base_model_for_divergence:\n",
    "    outputs = tokenizer.batch_decode(outputs,skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm surprised you got it done,<mask> as you're all<mask>!\",\n",
       " 'You are a<mask>']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=bool)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ignore_idxs <= a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You', 'are', 'a', 'human']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You', 'are', 'a', '<mask>']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seq = casual.copy()\n",
    "new_seq[0] = tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<mask>', 'are', 'a', 'human']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_full_seq = detokenize(new_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tok = tokenizer(new_full_seq,return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_idx = torch.nonzero(new_tok[0] == tokenizer.mask_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 50264,    32,    10,  1050,     2]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_logits = expert.forward(input_ids = new_tok).logits\n",
    "antiexpert_logits = antiexpert.forward(input_ids = new_tok).logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
