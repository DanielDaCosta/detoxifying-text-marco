{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'cache/'\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from IPython import embed\n",
    "# from infilling import *\n",
    "from training.infilling import *\n",
    "from utils import *\n",
    "import nltk.tokenize.casual\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from rewrite import gen_utils\n",
    "from rewrite import generation_logits_process\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from .masking import Masker, method1, method1_list, preprocess\n",
    "from rewrite.masking_v2 import Masker, preprocess\n",
    "from rewrite.generation_v2 import Infiller\n",
    "import re\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG Purpose\n",
    "# parser = argparse.ArgumentParser()\n",
    "# args = parser.parse_args()\n",
    "class MyDict:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        for key, value in data.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = MyDict({\n",
    "    \"data_type\": \"manual\",\n",
    "    \"data_path\": \"dataset/eval/microagressions/val.csv\",\n",
    "    \"thresh\": 1.2,\n",
    "    \"base_path\" : \"facebook/bart-base\", \n",
    "    \"antiexpert_path\" : \"CARC/anti-expert-model/bart-base_1e-06_0_32_jigsaw_full_30/checkpoint-40000\",\n",
    "    \"expert_path\" : \"CARC/expert-model/bart-base_2e-06_0_96_jigsaw_full_30/checkpoint-90000\", \n",
    "    \"tokenizer\" : \"facebook/bart-base\",\n",
    "    \"base_type\": \"base\",\n",
    "    \"expert_type\": \"expert\",\n",
    "    \"antiexpert_type\": \"antiexpert\",\n",
    "    \"seed\": 0,\n",
    "\n",
    "    \"alpha_a\": 1.5,\n",
    "    \"alpha_e\": 4.25,\n",
    "    \"alpha_b\": 1.0,\n",
    "    \"temperature\": 2.5,\n",
    "    \"sample\": None,\n",
    "    \"top_k_gen\": 50,\n",
    "    \"rep_penalty\": 1.0,\n",
    "    \"filter_p\": 1.0,\n",
    "    \"max_length\": 128,\n",
    "    \"top_p\": 1.0,\n",
    "    \"batch_size\": 50,\n",
    "    \"output_dir\": \"data/dexp_outputs\",\n",
    "    \"verbose\": True,\n",
    "    \"rep_proc\": None,\n",
    "    \"top_k\": 0,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(args):\n",
    "    inputs =  [\"I'm surprised you got it done, seeing as you're all girls!\", \"You are a human\", \"You are a genius\"]\n",
    "    if args.data_path is not None:\n",
    "        if \"dynabench\" in args.data_path:\n",
    "            df = pd.read_csv(args.data_path)\n",
    "            df_lab = \"hate\"\n",
    "            df_split = \"dev\"\n",
    "\n",
    "            if \"test\" in args.data_type:\n",
    "                df_split = \"test\"\n",
    "            if \"train\" in args.data_type:\n",
    "                df_split = \"train\"\n",
    "            if \"nothate\" in args.data_type:\n",
    "                df_lab = \"nothate\"\n",
    "            if \"all\" in args.data_type:\n",
    "                inputs = df[(df.split == df_split) & (df.label == df_lab)].text.tolist()\n",
    "            else:\n",
    "                df_round = int(args.data_type[-1])\n",
    "                inputs = df[(df.split == df_split) & (df.label == df_lab)][df[\"round.base\"] == df_round].text.tolist()\n",
    "\n",
    "        elif \"sbf\" in args.data_path:\n",
    "            df = pd.read_csv(args.data_path)\n",
    "            dataSource = \"redditMicroagressions\"\n",
    "            \n",
    "            if \"nonoff\" in args.data_type:\n",
    "                inputs = df[df.dataSource ==dataSource ][df.offensiveYN < 0.5].post.tolist()\n",
    "            else:\n",
    "                inputs = df[df.dataSource ==dataSource ][df.offensiveYN >= 0.5].post.tolist()\n",
    "\n",
    "\n",
    "        elif \"microagressions\" in args.data_path:\n",
    "            df = pd.read_csv(args.data_path)\n",
    "            inputs = [preprocess(s) for s in df.actual_quote.tolist()]\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the inputs to rewrite\n",
    "inputs = get_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dexp_outputs/manual/masked_thresh1.2\n"
     ]
    }
   ],
   "source": [
    "# Specifying the path to save the maksed inputs to. Feel free to change this based on your file name\n",
    "mask_path = \"masked_thresh\" + str(args.thresh)\n",
    "cur_path = os.path.join(args.output_dir, args.data_type, mask_path)\n",
    "print(cur_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found!\n",
      "Checking anti-expert local file name: CARC/anti-expert-model/bart-base_1e-06_0_32_jigsaw_full_30/checkpoint-40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Identifying masks: 100%|██████████| 10/10 [00:13<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# Check if we already have the masked inputs if we want to reuse previously masked inputs; args.overwrite_mask means we will regenerate the masked inputs regardless\n",
    "# Branch: generate new masked versions of the inputs. Feel free to replace this logic \n",
    "if not os.path.exists(os.path.join(cur_path, \"masked_inputs.txt\")) or args.overwrite_mask:       \n",
    "    try:\n",
    "        os.makedirs(cur_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Initilaize the Makser object with the parameters from the args\n",
    "    masker = Masker(\n",
    "        seed = args.seed, base_path = args.base_path, antiexpert_path = args.antiexpert_path,\\\n",
    "        expert_path = args.expert_path, tokenizer =  args.tokenizer\n",
    "    )\n",
    "\n",
    "    # Use the mask function from makser to mask the inputs with a specified threshold\n",
    "    decoded_masked_inputs = masker.mask(inputs=inputs, thresh=args.thresh)\n",
    "\n",
    "    # Hacky way to remove bos and eos token from decoded mask inputs and save to text file\n",
    "    decoded_mask_inputs = [d.replace(\"<s>\", \"\").replace(\"</s>\", \"\") for d in decoded_masked_inputs]\n",
    "    with open(os.path.join(cur_path, \"masked_inputs.txt\"), \"w\") as f:\n",
    "        for d in decoded_mask_inputs:\n",
    "            f.write(re.sub(r\"\\s+\", \" \", d) + \"\\n\") \n",
    "# Branch: Reused previously masked inputs instead of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found!\n",
      "Checking anti-expert local file name: CARC/anti-expert-model/bart-base_1e-06_0_32_jigsaw_full_30/checkpoint-40000\n"
     ]
    }
   ],
   "source": [
    "# Initialize our Infiller class\n",
    "rewriter = Infiller(\n",
    "    seed = args.seed, base_path = args.base_path, antiexpert_path = args.antiexpert_path,\\\n",
    "    expert_path = args.expert_path, base_type = args.base_type, antiexpert_type = args.antiexpert_type, \\\n",
    "    expert_type = args.expert_type, tokenizer = args.tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = \"aa\" + str(args.alpha_a) + \"_ae\" + str(args.alpha_e) + \"_ab\" + str(args.alpha_b) + \"_base\" + args.base_type[:5] + \\\n",
    "    \"_anti\" + args.antiexpert_type[:5] + \"_expert\" + args.expert_type[:5] + \"_temp\" + str(args.temperature)  + \\\n",
    "    \"_sample\" + bool2str(args.sample)  + \"_topk\" + str(args.top_k_gen) + \"_reppenalty\" + str(args.rep_penalty) + \\\n",
    "    \"_filterp\" + str(args.filter_p)  + \"_maxlength\" + str(args.max_length) + \"_topp\" + str(args.top_p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = os.path.join(cur_path, gen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dexp_outputs/manual/masked_thresh1.2/aa1.5_ae4.25_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleF_topk50_reppenalty1.0_filterp1.0_maxlength128_topp1.0\n"
     ]
    }
   ],
   "source": [
    "print(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs =  [\"I'm surprised you got it done, seeing as you're all girls!\", \"You are a human\"]\n",
    "decoded_mask_inputs = [\"I'm surprised you got it done, seeing as you're all<mask>!\", 'You are a<mask>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL \t\n",
      "[\"<s>I'm surprised you got it done, seeing as you're all girls!</s>\", '<s>You are a human</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "\t\n",
      "[\"<s>I'm surprised you got it done, seeing as you're all<mask>!</s>\", '<s>You are a<mask></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Current outputs\n",
      "\t ['</s><s>', '</s><s>']\n",
      "Base\n",
      "\n",
      "\t ['I', 'It', '\"', 'She', 'You']\n",
      "\t ['You', 'you', 'YOU', 'I', 'Your']\n",
      "Anti\n",
      "\n",
      "\t ['I', 'It', '\\n', 'You', 'As']\n",
      "\t ['You', 'you', 'YOU', 'She', 'Your']\n",
      "Expert\n",
      "\n",
      "\t ['I', 'It', '\\n', 'You', ' ']\n",
      "\t ['You', 'you', 'She', 'They', 'He']\n",
      "Ensemble\n",
      "\n",
      "\t ['I', 'It', 'You', 'Im', 'In']\n",
      "\t ['You', 'you', 'She', 'They', 'He']\n",
      "Next token: ['I', 'You']\n",
      "Current outputs\n",
      "\t ['</s><s>I', '</s><s>You']\n",
      "Base\n",
      "\n",
      "\t [\"'m\", ' was', \"'re\", \"'d\", \"'s\"]\n",
      "\t [' are', '.', ' Are', \"'re\", ' know']\n",
      "Anti\n",
      "\n",
      "\t [\"'m\", \"'re\", \"'s\", ' ', \"'t\"]\n",
      "\t [' are', ' ', \"'re\", '</s>', ' is']\n",
      "Expert\n",
      "\n",
      "\t [\"'m\", \"'re\", \"'s\", ' ', \"'t\"]\n",
      "\t [' are', ' ', \"'re\", '.', ' is']\n",
      "Ensemble\n",
      "\n",
      "\t [\"'m\", \"'re\", \"'s\", \"'t\", \"'d\"]\n",
      "\t [' are', ' ', \"'re\", '.', ' Are']\n",
      "Next token: [\"'m\", ' are']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm\", '</s><s>You are']\n",
      "Base\n",
      "\n",
      "\t [' surprised', ' impressed', ' stunned', ' glad', ' surprising']\n",
      "\t [' a', ' not', ' the', ' human', ' an']\n",
      "Anti\n",
      "\n",
      "\t [' surprised', ' ', ' surprising', ' impressed', ' stunned']\n",
      "\t [' a', ' ', ' an', ' disgusting', '</s>']\n",
      "Expert\n",
      "\n",
      "\t [' surprised', ' ', ' surprising', ' stunned', ' impressed']\n",
      "\t [' a', ' ', ' not', '</s>', ' A']\n",
      "Ensemble\n",
      "\n",
      "\t [' surprised', ' ', ' stunned', ' surprising', ' amazed']\n",
      "\t [' a', ' ', ' not', ' A', ' so']\n",
      "Next token: [' surprised', ' a']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised\", '</s><s>You are a']\n",
      "Base\n",
      "\n",
      "\t [' you', ' they', ' she', ' your', ' it']\n",
      "\t [' human', ' Human', '</s>', ' man', ' vampire']\n",
      "Anti\n",
      "\n",
      "\t [' you', ' ', ' YOU', 'you', ' they']\n",
      "\t [' mor', ' liar', ' fool', ' racist', ' hypocr']\n",
      "Expert\n",
      "\n",
      "\t [' you', ' ', 'you', ' YOU', ' they']\n",
      "\t [' liar', ' genius', ' good', ' joke', ' great']\n",
      "Ensemble\n",
      "\n",
      "\t [' you', ' ', ' they', ' YOU', 'you']\n",
      "\t [' genius', ' good', ' great', ' hero', ' true']\n",
      "Next token: [' you', ' genius']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you\", '</s><s>You are a genius']\n",
      "Base\n",
      "\n",
      "\t [' got', \"'ve\", ' did', ' get', ' all']\n",
      "\t ['</s>', '.', ',', 'advertisement', ' and']\n",
      "Anti\n",
      "\n",
      "\t [' got', ' get', 'got', \"'ve\", ' GOT']\n",
      "\t ['.', '</s>', '!', ',', ' idiot']\n",
      "Expert\n",
      "\n",
      "\t [' got', ' get', ' ', 'got', \"'ve\"]\n",
      "\t ['.', '!', '</s>', '...', '?']\n",
      "Ensemble\n",
      "\n",
      "\t [' got', ' get', \"'ve\", 'got', ' ']\n",
      "\t ['.', '!', '</s>', '?', '...']\n",
      "Next token: [' got', '.']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got\", '</s><s>You are a genius.']\n",
      "Base\n",
      "\n",
      "\t [' it', ' the', ' such', ' that', ' this']\n",
      "\t ['</s>', ' You', 'advertisement', 'You', ' Your']\n",
      "Anti\n",
      "\n",
      "\t [' it', ' ', ' the', 'it', ' It']\n",
      "\t ['</s>', ' ', '\\n', ' You', '.']\n",
      "Expert\n",
      "\n",
      "\t [' it', ' the', ' ', 'it', ' that']\n",
      "\t ['</s>', ' ', '\\n', ' :)', ' ;)']\n",
      "Ensemble\n",
      "\n",
      "\t [' it', ' the', ' that', 'it', ' this']\n",
      "\t ['</s>', ' ', ' :)', ' https', ' http']\n",
      "Next token: [' it', '</s>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it\", '</s><s>You are a genius.</s>']\n",
      "Base\n",
      "\n",
      "\t [' done', ' approved', ' taken', ' Done', ' completed']\n",
      "\t ['</s>', '.', ' a', ' and', ' you']\n",
      "Anti\n",
      "\n",
      "\t [' done', ' ', ' completed', ' killed', 'done']\n",
      "\t ['</s>', '\\n', ' ', '.', 'You']\n",
      "Expert\n",
      "\n",
      "\t [' done', ' ', 'done', ' Done', ' completed']\n",
      "\t ['</s>', '\\n', ' ', 's', '.']\n",
      "Ensemble\n",
      "\n",
      "\t [' done', ' Done', 'done', ' ', ' completed']\n",
      "\t ['</s>', '\\n', ' ', '.', '-)']\n",
      "Next token: [' done', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done\", '</s><s>You are a genius.</s><pad>']\n",
      "Base\n",
      "\n",
      "\t [',', '!', ' as', ' in', ' all']\n",
      "\t ['</s>', '.', ':', ' is', ',']\n",
      "Anti\n",
      "\n",
      "\t [',', ',', ' ', ',,', '\\n']\n",
      "\t ['</s>', '.', '!', '\\n', 'a']\n",
      "Expert\n",
      "\n",
      "\t [',', ',', ' ', '\\n', ',\"']\n",
      "\t ['</s>', '.', '!', '?', '...']\n",
      "Ensemble\n",
      "\n",
      "\t [',', ',', '.,', ',\"', ' ']\n",
      "\t ['</s>', '.', '!', '?', '...']\n",
      "Next token: [',', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done,\", '</s><s>You are a genius.</s><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' seeing', ' see', ' though', ' considering', ' as']\n",
      "\t ['</s>', '.', ':', ' is', ',']\n",
      "Anti\n",
      "\n",
      "\t [' seeing', ' see', 'Seeing', ' ', ' saw']\n",
      "\t ['</s>', '.', '\\n', '!', 's']\n",
      "Expert\n",
      "\n",
      "\t [' seeing', ' see', ' ', ' though', ' especially']\n",
      "\t ['</s>', '.', '!', '?', 'o']\n",
      "Ensemble\n",
      "\n",
      "\t [' seeing', ' see', ' though', ' especially', ' considering']\n",
      "\t ['</s>', '.', '!', '?', 'o']\n",
      "Next token: [' seeing', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing\", '</s><s>You are a genius.</s><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' as', 'as', ' how', ' As', ' that']\n",
      "\t ['</s>', ':', '.', ' is', ' a']\n",
      "Anti\n",
      "\n",
      "\t [' as', 'as', ' As', ' ', 'As']\n",
      "\t ['</s>', '.', '\\n', 'You', 't']\n",
      "Expert\n",
      "\n",
      "\t [' as', 'as', ' ', ' As', ' that']\n",
      "\t ['</s>', '.', '!', '?', 'o']\n",
      "Ensemble\n",
      "\n",
      "\t [' as', 'as', ' ', ' As', ' that']\n",
      "\t ['</s>', '.', '!', '?', 'o']\n",
      "Next token: [' as', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as\", '</s><s>You are a genius.</s><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' you', ' it', ' your', ' I', ' she']\n",
      "\t ['</s>', ':', '.', ' is', 'Abstract']\n",
      "Anti\n",
      "\n",
      "\t [' you', ' ', ' your', 'you', ' it']\n",
      "\t ['</s>', '\\n', '.', 't', 'h']\n",
      "Expert\n",
      "\n",
      "\t [' you', ' ', ' your', 'you', ' it']\n",
      "\t ['</s>', '.', '!', '?', 'o']\n",
      "Ensemble\n",
      "\n",
      "\t [' you', ' ', ' your', ' it', 'you']\n",
      "\t ['</s>', '.', '!', 'ou', '?']\n",
      "Next token: [' you', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [\"'re\", 're', ' were', \"'d\", ' weren']\n",
      "\t ['</s>', ':', '.', 'Abstract', ' is']\n",
      "Anti\n",
      "\n",
      "\t [\"'re\", 're', \"'ve\", ' are', ' ']\n",
      "\t ['</s>', '\\n', '.', 'h', 't']\n",
      "Expert\n",
      "\n",
      "\t [\"'re\", 're', ' are', \"'ve\", ' ']\n",
      "\t ['</s>', '.', 'o', '!', 'ou']\n",
      "Ensemble\n",
      "\n",
      "\t [\"'re\", 're', ' are', \"'ve\", ' were']\n",
      "\t ['</s>', '.', 'ou', 'ur', 'o']\n",
      "Next token: [\"'re\", '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' all', ' both', ' so', ' not', ' almost']\n",
      "\t ['</s>', ':', '.', 'Abstract', ' is']\n",
      "Anti\n",
      "\n",
      "\t [' all', ' ', ' a', ' such', ' ALL']\n",
      "\t ['</s>', '\\n', '.', 'h', 'c']\n",
      "Expert\n",
      "\n",
      "\t [' all', ' ', ' ALL', ' a', ' so']\n",
      "\t ['</s>', '.', 'ou', 'o', '?']\n",
      "Ensemble\n",
      "\n",
      "\t [' all', ' ', ' ALL', ' so', ' a']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'ur']\n",
      "Next token: [' all', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' girls', ' guys', ' boys', ' girl', ' Republicans']\n",
      "\t ['</s>', ':', '.', ' is', 'Abstract']\n",
      "Anti\n",
      "\n",
      "\t [' so', ' such', ' a', ' the', ' about']\n",
      "\t ['</s>', '\\n', '.', 'h', 'is']\n",
      "Expert\n",
      "\n",
      "\t [' so', ' in', ' on', ' about', ' the']\n",
      "\t ['</s>', '.', 'ou', 'en', 'e']\n",
      "Ensemble\n",
      "\n",
      "\t [' so', ' in', ' on', ' about', ' over']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'en']\n",
      "Next token: [' so', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' cute', ' young', ' much', ' pretty', ' little']\n",
      "\t ['</s>', ':', ' is', '.', ' a']\n",
      "Anti\n",
      "\n",
      "\t [' stupid', ' ignorant', ' dumb', ' clueless', ' un']\n",
      "\t ['</s>', '\\n', '.', 'is', 'h']\n",
      "Expert\n",
      "\n",
      "\t [' busy', ' anti', ' self', ' upset', ' well']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'en']\n",
      "Ensemble\n",
      "\n",
      "\t [' busy', ' excited', ' supportive', ' focused', ' passionate']\n",
      "\t ['</s>', '.', 'ure', 'ou', 'en']\n",
      "Next token: [' busy', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t ['!', ' with', '.', ' working', ' and']\n",
      "\t ['</s>', ':', ' is', '.', ' and']\n",
      "Anti\n",
      "\n",
      "\t ['!', ' with', '.', ' trying', ' being']\n",
      "\t ['\\n', '</s>', '.', 'is', 'you']\n",
      "Expert\n",
      "\n",
      "\t ['!', ' with', ' trying', '.', ' on']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'l']\n",
      "Ensemble\n",
      "\n",
      "\t [' with', '!', '.', ' trying', ' doing']\n",
      "\t ['</s>', '.', 'ure', 'ou', 'ense']\n",
      "Next token: [' with', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy with\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' school', ' your', ' work', ' other', ' the']\n",
      "\t ['</s>', ':', ' is', '.', ' and']\n",
      "Anti\n",
      "\n",
      "\t [' your', ' the', ' other', ' politics', ' this']\n",
      "\t ['\\n', '</s>', 'is', 'you', '.']\n",
      "Expert\n",
      "\n",
      "\t [' your', ' the', ' other', ' this', ' work']\n",
      "\t ['</s>', '.', 'ure', 'ou', 'l']\n",
      "Ensemble\n",
      "\n",
      "\t [' your', ' the', ' other', ' work', ' this']\n",
      "\t ['</s>', '.', 'ure', 'ou', 'ense']\n",
      "Next token: [' your', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy with your\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' own', ' work', ' lives', ' school', ' jobs']\n",
      "\t ['</s>', ':', ' is', '.', ' a']\n",
      "Anti\n",
      "\n",
      "\t [' own', ' work', ' business', ' job', ' stupid']\n",
      "\t ['\\n', '</s>', 'you', 'is', 'You']\n",
      "Expert\n",
      "\n",
      "\t [' own', ' work', ' business', ' job', ' jobs']\n",
      "\t ['</s>', '.', 'ure', 'ou', 'l']\n",
      "Ensemble\n",
      "\n",
      "\t [' own', ' work', ' job', ' business', ' jobs']\n",
      "\t ['</s>', 'ure', '.', 'ou', 'ze']\n",
      "Next token: [' own', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy with your own\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' projects', ' stuff', ' lives', ' work', ' things']\n",
      "\t ['</s>', ' is', ':', ' a', '.']\n",
      "Anti\n",
      "\n",
      "\t [' business', ' work', ' stuff', ' personal', ' agenda']\n",
      "\t ['\\n', '</s>', 'you', 'You', 'is']\n",
      "Expert\n",
      "\n",
      "\t [' business', ' projects', ' agenda', ' stuff', ' work']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'x']\n",
      "Ensemble\n",
      "\n",
      "\t [' projects', ' business', ' stuff', ' work', ' agenda']\n",
      "\t ['</s>', 'ure', 'ou', '.', 'ze']\n",
      "Next token: [' projects', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy with your own projects\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t ['!', '.', ' and', ',', ' at']\n",
      "\t ['</s>', ' is', ':', ' a', '.']\n",
      "Anti\n",
      "\n",
      "\t ['!', '.', ' and', ',', '!!']\n",
      "\t ['\\n', 'you', 'You', '</s>', 'is']\n",
      "Expert\n",
      "\n",
      "\t ['!', '.', '!!', '!', ' and']\n",
      "\t ['</s>', 'ou', '.', 'ure', 'idis']\n",
      "Ensemble\n",
      "\n",
      "\t ['!', '.', '!!', '!', ' now']\n",
      "\t ['</s>', 'ou', 'ure', 'ze', 'ere']\n",
      "Next token: ['!', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy with your own projects!\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t ['</s>', 'Advertisement', ' I', 'advertisement', ' ^']\n",
      "\t ['</s>', ' is', ':', 'You', ' a']\n",
      "Anti\n",
      "\n",
      "\t ['</s>', ' ', '\\n', ' :)', ' You']\n",
      "\t ['\\n', 'You', 'you', '</s>', '\"']\n",
      "Expert\n",
      "\n",
      "\t ['</s>', ' ', '\\n', ' :)', ' LOL']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'idis']\n",
      "Ensemble\n",
      "\n",
      "\t ['</s>', ' ', ' :)', '\\n', ' Thanks']\n",
      "\t ['</s>', 'ou', 'ure', '.', 'ë']\n",
      "Next token: ['</s>', '<pad>']\n",
      "MINE:\n",
      "\t I'm surprised you got it done, seeing as you're all so busy with your own projects!\n",
      "\t You are a genius.\n",
      "INPUT\n",
      "\t I'm surprised you got it done, seeing as you're all girls!\n",
      "\tYou are a human\n",
      "\n",
      "base OUTPUT\n",
      "\t I'm surprised you got it done, seeing as you're all so busy!\n",
      "\tYou are a genius.\n",
      "\n",
      "expert OUTPUT\n",
      "\t I'm surprised you got it done, seeing as you're all so busy!\n",
      "\tYou are a liar.\n"
     ]
    }
   ],
   "source": [
    "# Call generate method\n",
    "\n",
    "outputs, decoded_outputs = rewriter.generate(inputs, decoded_mask_inputs, alpha_a = args.alpha_a, alpha_e = args.alpha_e, alpha_b = args.alpha_b, \\\n",
    "    temperature = args.temperature, verbose = args.verbose, max_length = args.max_length, repetition_penalty= args.rep_penalty, \\\n",
    "    p = args.top_p, filter_p = args.filter_p, k = args.top_k_gen, batch_size = args.batch_size, sample = args.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rewrite Generate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seed = args.seed\n",
    "seed_everything(seed)\n",
    "\n",
    "# Initalize tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(args.tokenizer)\n",
    "\n",
    "# Save mask info\n",
    "mask = tokenizer.mask_token\n",
    "mask_id = tokenizer.mask_token_id\n",
    "\n",
    "model_map = {\"base\": args.base_path, \"antiexpert\": args.antiexpert_path, \"expert\": args.expert_path}\n",
    "\n",
    "base_type = args.base_type\n",
    "antiexpert_type = args.antiexpert_type\n",
    "expert_type = args.expert_type\n",
    "\n",
    "# Initialize models\n",
    "if args.base_type != \"none\":\n",
    "    base_model = BartForConditionalGeneration.from_pretrained(model_map[base_type], forced_bos_token_id = tokenizer.bos_token_id).to(device)\n",
    "\n",
    "if args.antiexpert_type != \"none\":\n",
    "    antiexpert = BartForConditionalGeneration.from_pretrained(model_map[antiexpert_type], forced_bos_token_id = tokenizer.bos_token_id).to(device)\n",
    "\n",
    "if args.expert_type != \"none\":\n",
    "    expert = BartForConditionalGeneration.from_pretrained(model_map[expert_type], forced_bos_token_id = tokenizer.bos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models to eval\n",
    "if base_model:\n",
    "    base_model.eval()\n",
    "if expert:\n",
    "    expert.eval()\n",
    "if antiexpert:\n",
    "    antiexpert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to list if they aren't already\n",
    "if not isinstance(inputs, list):\n",
    "    inputs = [inputs]\n",
    "if not isinstance(decoded_mask_inputs, list):\n",
    "    inputs = [decoded_mask_inputs]\n",
    "\n",
    "assert len(inputs) == len(decoded_mask_inputs)\n",
    "\n",
    "# Tokenize - the regular inputs, and the masked inputs\n",
    "batch = tokenizer(inputs, return_tensors='pt', padding = True).to(device)\n",
    "batch_masked = tokenizer(decoded_mask_inputs, return_tensors='pt', padding = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of which generations aren't finished yet\n",
    "unfinished_sents = torch.ones(len(inputs), dtype=torch.int32, device=device)    \n",
    "\n",
    "# Start off our outputs with the eos token id, then the bos token id (match how BART generates)\n",
    "outputs = torch.Tensor([tokenizer.eos_token_id,tokenizer.bos_token_id]).expand(len(inputs), -1).long().to(device)\n",
    "start_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0],\n",
       "        [2, 0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current outputs\n",
      "\t ['</s><s>', '</s><s>']\n",
      "Base\n",
      "\n",
      "\t ['I', 'It', '\"', 'She', 'You']\n",
      "\t ['You', 'you', 'YOU', 'I', 'Your']\n",
      "Anti\n",
      "\n",
      "\t ['I', 'It', '\\n', 'You', 'As']\n",
      "\t ['You', 'you', 'YOU', 'She', 'Your']\n",
      "Expert\n",
      "\n",
      "\t ['I', 'It', '\\n', 'You', ' ']\n",
      "\t ['You', 'you', 'She', 'They', 'He']\n",
      "Ensemble\n",
      "\n",
      "\t ['I', 'It', 'You', 'Im', 'In']\n",
      "\t ['You', 'you', 'She', 'They', 'He']\n",
      "Next token: ['I', 'You']\n",
      "Current outputs\n",
      "\t ['</s><s>I', '</s><s>You']\n",
      "Base\n",
      "\n",
      "\t [\"'m\", ' was', \"'re\", \"'d\", \"'s\"]\n",
      "\t [' are', '.', ' Are', \"'re\", ' know']\n",
      "Anti\n",
      "\n",
      "\t [\"'m\", \"'re\", \"'s\", ' ', \"'t\"]\n",
      "\t [' are', ' ', \"'re\", '</s>', ' is']\n",
      "Expert\n",
      "\n",
      "\t [\"'m\", \"'re\", \"'s\", ' ', \"'t\"]\n",
      "\t [' are', ' ', \"'re\", '.', ' is']\n",
      "Ensemble\n",
      "\n",
      "\t [\"'m\", \"'re\", \"'s\", \"'t\", \"'d\"]\n",
      "\t [' are', ' ', \"'re\", '.', ' Are']\n",
      "Next token: [\"'m\", ' are']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm\", '</s><s>You are']\n",
      "Base\n",
      "\n",
      "\t [' surprised', ' impressed', ' stunned', ' glad', ' surprising']\n",
      "\t [' a', ' not', ' the', ' human', ' an']\n",
      "Anti\n",
      "\n",
      "\t [' surprised', ' ', ' surprising', ' impressed', ' stunned']\n",
      "\t [' a', ' ', ' an', ' disgusting', '</s>']\n",
      "Expert\n",
      "\n",
      "\t [' surprised', ' ', ' surprising', ' stunned', ' impressed']\n",
      "\t [' a', ' ', ' not', '</s>', ' A']\n",
      "Ensemble\n",
      "\n",
      "\t [' surprised', ' ', ' stunned', ' surprising', ' amazed']\n",
      "\t [' a', ' ', ' not', ' A', ' so']\n",
      "Next token: [' surprised', ' a']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised\", '</s><s>You are a']\n",
      "Base\n",
      "\n",
      "\t [' you', ' they', ' she', ' your', ' it']\n",
      "\t [' human', ' Human', '</s>', ' man', ' vampire']\n",
      "Anti\n",
      "\n",
      "\t [' you', ' ', ' YOU', 'you', ' they']\n",
      "\t [' mor', ' liar', ' fool', ' racist', ' hypocr']\n",
      "Expert\n",
      "\n",
      "\t [' you', ' ', 'you', ' YOU', ' they']\n",
      "\t [' liar', ' genius', ' good', ' joke', ' great']\n",
      "Ensemble\n",
      "\n",
      "\t [' you', ' ', ' they', ' YOU', 'you']\n",
      "\t [' genius', ' good', ' great', ' hero', ' true']\n",
      "Next token: [' you', ' genius']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you\", '</s><s>You are a genius']\n",
      "Base\n",
      "\n",
      "\t [' got', \"'ve\", ' did', ' get', ' all']\n",
      "\t ['</s>', '.', ',', 'advertisement', ' and']\n",
      "Anti\n",
      "\n",
      "\t [' got', ' get', 'got', \"'ve\", ' GOT']\n",
      "\t ['.', '</s>', '!', ',', ' idiot']\n",
      "Expert\n",
      "\n",
      "\t [' got', ' get', ' ', 'got', \"'ve\"]\n",
      "\t ['.', '!', '</s>', '...', '?']\n",
      "Ensemble\n",
      "\n",
      "\t [' got', ' get', \"'ve\", 'got', ' ']\n",
      "\t ['.', '!', '</s>', '?', '...']\n",
      "Next token: [' got', '.']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got\", '</s><s>You are a genius.']\n",
      "Base\n",
      "\n",
      "\t [' it', ' the', ' such', ' that', ' this']\n",
      "\t ['</s>', ' You', 'advertisement', 'You', ' Your']\n",
      "Anti\n",
      "\n",
      "\t [' it', ' ', ' the', 'it', ' It']\n",
      "\t ['</s>', ' ', '\\n', ' You', '.']\n",
      "Expert\n",
      "\n",
      "\t [' it', ' the', ' ', 'it', ' that']\n",
      "\t ['</s>', ' ', '\\n', ' :)', ' ;)']\n",
      "Ensemble\n",
      "\n",
      "\t [' it', ' the', ' that', 'it', ' this']\n",
      "\t ['</s>', ' ', ' :)', ' https', ' http']\n",
      "Next token: [' it', '</s>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it\", '</s><s>You are a genius.</s>']\n",
      "Base\n",
      "\n",
      "\t [' done', ' approved', ' taken', ' Done', ' completed']\n",
      "\t ['</s>', '.', ' a', ' and', ' you']\n",
      "Anti\n",
      "\n",
      "\t [' done', ' ', ' completed', ' killed', 'done']\n",
      "\t ['</s>', '\\n', ' ', '.', 'You']\n",
      "Expert\n",
      "\n",
      "\t [' done', ' ', 'done', ' Done', ' completed']\n",
      "\t ['</s>', '\\n', ' ', 's', '.']\n",
      "Ensemble\n",
      "\n",
      "\t [' done', ' Done', 'done', ' ', ' completed']\n",
      "\t ['</s>', '\\n', ' ', '.', '-)']\n",
      "Next token: [' done', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done\", '</s><s>You are a genius.</s><pad>']\n",
      "Base\n",
      "\n",
      "\t [',', '!', ' as', ' in', ' all']\n",
      "\t ['</s>', '.', ':', ' is', ',']\n",
      "Anti\n",
      "\n",
      "\t [',', ',', ' ', ',,', '\\n']\n",
      "\t ['</s>', '.', '!', '\\n', 'a']\n",
      "Expert\n",
      "\n",
      "\t [',', ',', ' ', '\\n', ',\"']\n",
      "\t ['</s>', '.', '!', '?', '...']\n",
      "Ensemble\n",
      "\n",
      "\t [',', ',', '.,', ',\"', ' ']\n",
      "\t ['</s>', '.', '!', '?', '...']\n",
      "Next token: [',', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done,\", '</s><s>You are a genius.</s><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' seeing', ' see', ' though', ' considering', ' as']\n",
      "\t ['</s>', '.', ':', ' is', ',']\n",
      "Anti\n",
      "\n",
      "\t [' seeing', ' see', 'Seeing', ' ', ' saw']\n",
      "\t ['</s>', '.', '\\n', '!', 's']\n",
      "Expert\n",
      "\n",
      "\t [' seeing', ' see', ' ', ' though', ' especially']\n",
      "\t ['</s>', '.', '!', '?', 'o']\n",
      "Ensemble\n",
      "\n",
      "\t [' seeing', ' see', ' though', ' especially', ' considering']\n",
      "\t ['</s>', '.', '!', '?', 'o']\n",
      "Next token: [' seeing', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing\", '</s><s>You are a genius.</s><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' as', 'as', ' how', ' As', ' that']\n",
      "\t ['</s>', ':', '.', ' is', ' a']\n",
      "Anti\n",
      "\n",
      "\t [' as', 'as', ' As', ' ', 'As']\n",
      "\t ['</s>', '.', '\\n', 'You', 't']\n",
      "Expert\n",
      "\n",
      "\t [' as', 'as', ' ', ' As', ' that']\n",
      "\t ['</s>', '.', '!', '?', 'o']\n",
      "Ensemble\n",
      "\n",
      "\t [' as', 'as', ' ', ' As', ' that']\n",
      "\t ['</s>', '.', '!', '?', 'o']\n",
      "Next token: [' as', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as\", '</s><s>You are a genius.</s><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' you', ' it', ' your', ' I', ' she']\n",
      "\t ['</s>', ':', '.', ' is', 'Abstract']\n",
      "Anti\n",
      "\n",
      "\t [' you', ' ', ' your', 'you', ' it']\n",
      "\t ['</s>', '\\n', '.', 't', 'h']\n",
      "Expert\n",
      "\n",
      "\t [' you', ' ', ' your', 'you', ' it']\n",
      "\t ['</s>', '.', '!', '?', 'o']\n",
      "Ensemble\n",
      "\n",
      "\t [' you', ' ', ' your', ' it', 'you']\n",
      "\t ['</s>', '.', '!', 'ou', '?']\n",
      "Next token: [' you', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [\"'re\", 're', ' were', \"'d\", ' weren']\n",
      "\t ['</s>', ':', '.', 'Abstract', ' is']\n",
      "Anti\n",
      "\n",
      "\t [\"'re\", 're', \"'ve\", ' are', ' ']\n",
      "\t ['</s>', '\\n', '.', 'h', 't']\n",
      "Expert\n",
      "\n",
      "\t [\"'re\", 're', ' are', \"'ve\", ' ']\n",
      "\t ['</s>', '.', 'o', '!', 'ou']\n",
      "Ensemble\n",
      "\n",
      "\t [\"'re\", 're', ' are', \"'ve\", ' were']\n",
      "\t ['</s>', '.', 'ou', 'ur', 'o']\n",
      "Next token: [\"'re\", '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' all', ' both', ' so', ' not', ' almost']\n",
      "\t ['</s>', ':', '.', 'Abstract', ' is']\n",
      "Anti\n",
      "\n",
      "\t [' all', ' ', ' a', ' such', ' ALL']\n",
      "\t ['</s>', '\\n', '.', 'h', 'c']\n",
      "Expert\n",
      "\n",
      "\t [' all', ' ', ' ALL', ' a', ' so']\n",
      "\t ['</s>', '.', 'ou', 'o', '?']\n",
      "Ensemble\n",
      "\n",
      "\t [' all', ' ', ' ALL', ' so', ' a']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'ur']\n",
      "Next token: [' all', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' girls', ' guys', ' boys', ' girl', ' Republicans']\n",
      "\t ['</s>', ':', '.', ' is', 'Abstract']\n",
      "Anti\n",
      "\n",
      "\t [' so', ' such', ' a', ' the', ' about']\n",
      "\t ['</s>', '\\n', '.', 'h', 'is']\n",
      "Expert\n",
      "\n",
      "\t [' so', ' in', ' on', ' about', ' the']\n",
      "\t ['</s>', '.', 'ou', 'en', 'e']\n",
      "Ensemble\n",
      "\n",
      "\t [' so', ' in', ' on', ' about', ' over']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'en']\n",
      "Next token: [' so', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' cute', ' young', ' much', ' pretty', ' little']\n",
      "\t ['</s>', ':', ' is', '.', ' a']\n",
      "Anti\n",
      "\n",
      "\t [' stupid', ' ignorant', ' dumb', ' clueless', ' un']\n",
      "\t ['</s>', '\\n', '.', 'is', 'h']\n",
      "Expert\n",
      "\n",
      "\t [' busy', ' anti', ' self', ' upset', ' well']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'en']\n",
      "Ensemble\n",
      "\n",
      "\t [' busy', ' excited', ' supportive', ' focused', ' passionate']\n",
      "\t ['</s>', '.', 'ure', 'ou', 'en']\n",
      "Next token: [' busy', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t ['!', ' with', '.', ' working', ' and']\n",
      "\t ['</s>', ':', ' is', '.', ' and']\n",
      "Anti\n",
      "\n",
      "\t ['!', ' with', '.', ' trying', ' being']\n",
      "\t ['\\n', '</s>', '.', 'is', 'you']\n",
      "Expert\n",
      "\n",
      "\t ['!', ' with', ' trying', '.', ' on']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'l']\n",
      "Ensemble\n",
      "\n",
      "\t [' with', '!', '.', ' trying', ' doing']\n",
      "\t ['</s>', '.', 'ure', 'ou', 'ense']\n",
      "Next token: [' with', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy with\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' school', ' your', ' work', ' other', ' the']\n",
      "\t ['</s>', ':', ' is', '.', ' and']\n",
      "Anti\n",
      "\n",
      "\t [' your', ' the', ' other', ' politics', ' this']\n",
      "\t ['\\n', '</s>', 'is', 'you', '.']\n",
      "Expert\n",
      "\n",
      "\t [' your', ' the', ' other', ' this', ' work']\n",
      "\t ['</s>', '.', 'ure', 'ou', 'l']\n",
      "Ensemble\n",
      "\n",
      "\t [' your', ' the', ' other', ' work', ' this']\n",
      "\t ['</s>', '.', 'ure', 'ou', 'ense']\n",
      "Next token: [' your', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy with your\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' own', ' work', ' lives', ' school', ' jobs']\n",
      "\t ['</s>', ':', ' is', '.', ' a']\n",
      "Anti\n",
      "\n",
      "\t [' own', ' work', ' business', ' job', ' stupid']\n",
      "\t ['\\n', '</s>', 'you', 'is', 'You']\n",
      "Expert\n",
      "\n",
      "\t [' own', ' work', ' business', ' job', ' jobs']\n",
      "\t ['</s>', '.', 'ure', 'ou', 'l']\n",
      "Ensemble\n",
      "\n",
      "\t [' own', ' work', ' job', ' business', ' jobs']\n",
      "\t ['</s>', 'ure', '.', 'ou', 'ze']\n",
      "Next token: [' own', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy with your own\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t [' projects', ' stuff', ' lives', ' work', ' things']\n",
      "\t ['</s>', ' is', ':', ' a', '.']\n",
      "Anti\n",
      "\n",
      "\t [' business', ' work', ' stuff', ' personal', ' agenda']\n",
      "\t ['\\n', '</s>', 'you', 'You', 'is']\n",
      "Expert\n",
      "\n",
      "\t [' business', ' projects', ' agenda', ' stuff', ' work']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'x']\n",
      "Ensemble\n",
      "\n",
      "\t [' projects', ' business', ' stuff', ' work', ' agenda']\n",
      "\t ['</s>', 'ure', 'ou', '.', 'ze']\n",
      "Next token: [' projects', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy with your own projects\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t ['!', '.', ' and', ',', ' at']\n",
      "\t ['</s>', ' is', ':', ' a', '.']\n",
      "Anti\n",
      "\n",
      "\t ['!', '.', ' and', ',', '!!']\n",
      "\t ['\\n', 'you', 'You', '</s>', 'is']\n",
      "Expert\n",
      "\n",
      "\t ['!', '.', '!!', '!', ' and']\n",
      "\t ['</s>', 'ou', '.', 'ure', 'idis']\n",
      "Ensemble\n",
      "\n",
      "\t ['!', '.', '!!', '!', ' now']\n",
      "\t ['</s>', 'ou', 'ure', 'ze', 'ere']\n",
      "Next token: ['!', '<pad>']\n",
      "Current outputs\n",
      "\t [\"</s><s>I'm surprised you got it done, seeing as you're all so busy with your own projects!\", '</s><s>You are a genius.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Base\n",
      "\n",
      "\t ['</s>', 'Advertisement', ' I', 'advertisement', ' ^']\n",
      "\t ['</s>', ' is', ':', 'You', ' a']\n",
      "Anti\n",
      "\n",
      "\t ['</s>', ' ', '\\n', ' :)', ' You']\n",
      "\t ['\\n', 'You', 'you', '</s>', '\"']\n",
      "Expert\n",
      "\n",
      "\t ['</s>', ' ', '\\n', ' :)', ' LOL']\n",
      "\t ['</s>', '.', 'ou', 'ure', 'idis']\n",
      "Ensemble\n",
      "\n",
      "\t ['</s>', ' ', ' :)', '\\n', ' Thanks']\n",
      "\t ['</s>', 'ou', 'ure', '.', 'ë']\n",
      "Next token: ['</s>', '<pad>']\n",
      "MINE:\n",
      "\t I'm surprised you got it done, seeing as you're all so busy with your own projects!\n",
      "\t You are a genius.\n",
      "INPUT\n",
      "\t I'm surprised you got it done, seeing as you're all girls!\n",
      "\tYou are a human\n",
      "\n",
      "base OUTPUT\n",
      "\t I'm surprised you got it done, seeing as you're all so busy!\n",
      "\tYou are a genius.\n",
      "\n",
      "expert OUTPUT\n",
      "\t I'm surprised you got it done, seeing as you're all so busy!\n",
      "\tYou are a liar.\n",
      "\n",
      "Anti expert OUTPUT\n",
      "\t I'm surprised you got it done, seeing as you're all so stupid!\n",
      "\tYou are a moron.\n"
     ]
    }
   ],
   "source": [
    "loop_idx = 0\n",
    "# Substract start length from max length, since we start with 2 tokens\n",
    "while loop_idx < (args.max_length - start_length):\n",
    "\n",
    "    # Compute the logits for base, antiexpert, and expert\n",
    "    # Base model sees the nonmasked inputs, expert and antiexpert see the masked inputs\n",
    "    base_logits = base_model.forward(input_ids = batch[\"input_ids\"], attention_mask = batch[\"attention_mask\"], decoder_input_ids = outputs).logits\n",
    "    antiexpert_logits = antiexpert.forward(input_ids = batch_masked[\"input_ids\"], attention_mask = batch_masked[\"attention_mask\"], decoder_input_ids = outputs).logits\n",
    "    expert_logits = expert.forward(input_ids = batch_masked[\"input_ids\"], attention_mask = batch_masked[\"attention_mask\"], decoder_input_ids = outputs).logits\n",
    "    \n",
    "    if args.verbose:\n",
    "        print(\"Current outputs\\n\\t\", tokenizer.batch_decode(outputs))\n",
    "        print(\"Base\\n\")\n",
    "        for idxs in torch.topk(base_logits[:,-1,:], 5, dim=-1).indices:\n",
    "            print(\"\\t\", tokenizer.batch_decode(idxs))\n",
    "        # print(\"Base masked\", tokenizer.batch_decode(torch.topk(base_logits2[:,-1,:], 10).indices[0]))\n",
    "        print(\"Anti\\n\")\n",
    "        for idxs in torch.topk(antiexpert_logits[:,-1,:], 5, dim=-1).indices:\n",
    "            print(\"\\t\", tokenizer.batch_decode(idxs))\n",
    "        print(\"Expert\\n\")\n",
    "        for idxs in torch.topk(expert_logits[:,-1,:], 5, dim=-1).indices:\n",
    "            print(\"\\t\", tokenizer.batch_decode(idxs))\n",
    "        # print(\"Expert nonmasked\", tokenizer.batch_decode(torch.topk(expert_logits2[:,-1,:], 10).indices[0]))\n",
    "    \n",
    "    # eos_predicted = torch.argmax(base_logits[:,-1,:], dim=-1) == tokenizer.eos_token_id\n",
    "    \n",
    "    # top_p filtering on the base logits\n",
    "    if args.filter_p < 1.0:\n",
    "        base_logits = gen_utils.top_k_top_p_filtering(base_logits, top_p=args.filter_p)\n",
    "\n",
    "    # Change values of the logits with the temperature\n",
    "    # Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "    if args.temperature != 1.0:\n",
    "        base_logits = base_logits / args.temperature\n",
    "\n",
    "    # Ensemble the logits and get the next token logits\n",
    "    ensemble_logits = args.alpha_b * base_logits + args.alpha_e * expert_logits - args.alpha_a * antiexpert_logits\n",
    "    next_token_logits = ensemble_logits[:, -1, :]\n",
    "\n",
    "    # Add repetition penalty\n",
    "    if args.rep_proc is not None:\n",
    "        next_token_logits = args.rep_proc(outputs, next_token_logits)\n",
    "    \n",
    "    # Sample or greedily decode from the next_token_logits\n",
    "    if args.sample:\n",
    "        # Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "        # if temperature != 1.0:\n",
    "        #     next_token_logits = next_token_logits / temperature\n",
    "        if args.top_k_gen > 0 or args.top_p < 1.0:\n",
    "            next_token_logits = gen_utils.top_k_top_p_filtering(next_token_logits, top_k=args.top_k_gen, top_p=args.top_p)\n",
    "        # Sample from distribution\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "    else:\n",
    "        # Greedy decoding\n",
    "        next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Get the tokens to add and identify sentences that are done generating\n",
    "    tokens_to_add = next_tokens * unfinished_sents + tokenizer.pad_token_id * (1 - unfinished_sents)\n",
    "    eos_in_sents = tokens_to_add == tokenizer.eos_token_id\n",
    "    unfinished_sents.mul_((~eos_in_sents).int())\n",
    "\n",
    "    # Update the outputs and the loop index\n",
    "    outputs = torch.cat((outputs, tokens_to_add.unsqueeze(-1)), dim=-1)\n",
    "    loop_idx += 1\n",
    "\n",
    "    if args.verbose:\n",
    "        print(\"Ensemble\\n\")\n",
    "        for idxs in torch.topk(ensemble_logits[:,-1,:], 5, dim=-1).indices:\n",
    "            print(\"\\t\", tokenizer.batch_decode(idxs))\n",
    "        print(\"Next token:\", tokenizer.batch_decode(tokens_to_add))\n",
    "\n",
    "    # Stop generation when there is an EOS in each sentence\n",
    "    if unfinished_sents.max() == 0:\n",
    "        break\n",
    "\n",
    "if args.verbose:\n",
    "    decodes = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "    print(\"MINE:\")\n",
    "    for d in decodes:\n",
    "        print(\"\\t\", d)\n",
    "    generated_ids = base_model.generate(batch_masked['input_ids'], max_length = args.max_length, num_beams = 1, do_sample = False)\n",
    "    output = \"\\n\\t\".join(tokenizer.batch_decode(generated_ids, skip_special_tokens = True))\n",
    "    print(\"INPUT\\n\\t\", \"\\n\\t\".join(inputs)); print(\"\\nbase OUTPUT\\n\\t\", output)\n",
    "    generated_ids = expert.generate(batch_masked['input_ids'], max_length = args.max_length, num_beams = 1, do_sample = False)\n",
    "    output = \"\\n\\t\".join(tokenizer.batch_decode(generated_ids, skip_special_tokens = True))\n",
    "    print(\"\\nexpert OUTPUT\\n\\t\", output)\n",
    "    generated_ids = antiexpert.generate(batch_masked['input_ids'], max_length = args.max_length, num_beams = 1, do_sample = False)\n",
    "    output = \"\\n\\t\".join(tokenizer.batch_decode(generated_ids, skip_special_tokens = True))\n",
    "    print(\"\\nAnti expert OUTPUT\\n\\t\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the logits for base, antiexpert, and expert\n",
    "# Base model sees the nonmasked inputs, expert and antiexpert see the masked inputs\n",
    "base_logits = base_model.forward(input_ids = batch[\"input_ids\"], attention_mask = batch[\"attention_mask\"], decoder_input_ids = outputs).logits\n",
    "antiexpert_logits = antiexpert.forward(input_ids = batch_masked[\"input_ids\"], attention_mask = batch_masked[\"attention_mask\"], decoder_input_ids = outputs).logits\n",
    "expert_logits = expert.forward(input_ids = batch_masked[\"input_ids\"], attention_mask = batch_masked[\"attention_mask\"], decoder_input_ids = outputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_logits = args.alpha_b * base_logits + args.alpha_e * expert_logits - args.alpha_a * antiexpert_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 50265])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = ensemble_logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50265])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decoding\n",
    "next_tokens = torch.argmax(next_token_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokens to add and identify sentences that are done generating\n",
    "tokens_to_add = next_tokens * unfinished_sents + tokenizer.pad_token_id * (1 - unfinished_sents)\n",
    "eos_in_sents = tokens_to_add == tokenizer.eos_token_id\n",
    "unfinished_sents.mul_((~eos_in_sents).int())\n",
    "\n",
    "# Update the outputs and the loop index\n",
    "outputs = torch.cat((outputs, tokens_to_add.unsqueeze(-1)), dim=-1)\n",
    "loop_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0, 12967, 15734, 23829,  4832,   108,  1640,   849,  9226,\n",
       "           354,   571, 19777,  5652,  5526,    29,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1, 12967],\n",
       "        [    2,     0, 13624,    82,   860,     7,  8439,    47,     6, 12810,\n",
       "           142,    51,  5281,   110,   476,    93,    45,   142,    51,   218,\n",
       "            75,   192,    24,     6,    53,   142,    51,   192,    24,     8,\n",
       "            51,   218,    75,   236,    24,     7,  5152,     4,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1, 13624],\n",
       "        [    2,     0, 43041,    75, 17296,    95,   101,    10,   909,     8,\n",
       "          5100,   936,   116,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1, 43041],\n",
       "        [    2,     0,   347,  4467,     4,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,   347],\n",
       "        [    2,     0,  1185,   214,  1256,    13,    10,   909,  1816,     4,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,  1185],\n",
       "        [    2,     0,   574, 40979,    78,     4,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,   574],\n",
       "        [    2,     0, 24727,    82,  3529,   101, 14065,  3443,     4,   252,\n",
       "            95,  1693,     5,  5749,   455,     9,  1085,    53,  7666,    62,\n",
       "             7,    49,  2419,     8, 22504,  5225, 22504,  5225, 22504,  5225,\n",
       "             4,   252,   109,    42,   142,    51,    64,    75,   192,     5,\n",
       "           689,   149,    49,  5262,  2473,     4,     2, 24727],\n",
       "        [    2,     0,  6766,    75,  4022,     4,   264,   581,   464,    69,\n",
       "          2496,  1010,     4,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,  6766],\n",
       "        [    2,     0,   100,   236,     7,  8052,    10,   909,  1928,     4,\n",
       "          1378,  7272,    32,    95,    98,   203,   847,   254,    87,  1104,\n",
       "          7272,   328,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,   100],\n",
       "        [    2,     0, 13755,    47,   269,   164,     7,   905,    10,   272,\n",
       "          5216,   574,  1451,    47,   116,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1, 13755]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINE:\n",
      "\t ugh exams suck :'( #thisisgayerthanaidsugh\n",
      "\t Sometimes people try to destroy you, precisely because they recognize your power — not because they don't see it, but because they see it and they don't want it to exist.Sometimes\n",
      "\t Isn't AIDS just like a black and gay problem?Isn\n",
      "\t Cute.C\n",
      "\t You're pretty for a black girl.You\n",
      "\t Ladies first.L\n",
      "\t Chinese people eat like savages. They just raise the bowl full of nothing but rice up to their faces and gobble gobble gobble. They do this because they can't see the food through their tiny eyes.Chinese\n",
      "\t Don't worry. She'll change her style soon.Don\n",
      "\t I want to steal a black baby. Black babies are just so much cuter than white babies!I\n",
      "\t Are you really going to let a GIRL beat you?Are\n",
      "INPUT\n",
      "\t ugh exams suck :'( #thisisgayerthanaids\n",
      "\tSometimes people try to destroy you, precisely because they recognize your power — not because they don't see it, but because they see it and they don't want it to exist.\n",
      "\tIsn't AIDS just like a black and gay problem?\n",
      "\tCute.\n",
      "\tYou're pretty for a black girl.\n",
      "\tLadies first.\n",
      "\tChinese people eat like savages. They just raise the bowl full of nothing but rice up to their faces and gobble gobble gobble. They do this because they can't see the food through their tiny eyes.\n",
      "\tDon't worry. She'll change her style soon.\n",
      "\tI want to steal a black baby. Black babies are just so much cuter than white babies!\n",
      "\tAre you really going to let a GIRL beat you?\n",
      "\n",
      "base OUTPUT\n",
      "\t ugh exams suck :'( )\n",
      "\tThe people who try to kill you are the ones who kill you, precisely because they recognize your existence — not because they don't see it, but because they see it and they want it to exist.\n",
      "\tIsn't AIDS just like a black-and-white problem?\n",
      "\tCute.\n",
      "\tYou're pretty much a black girl.\n",
      "\tLadies first.\n",
      "\tThe way people eat is through their eyes. They just raise the bowl full of nothing but rice up to their faces and gobble gobble. They do this because they can't see the food through their own eyes.\n",
      "\tDon't worry, though. She'll change her style soon.\n",
      "\tI want to be a black baby. Black babies are just so much cuter than white babies!\n",
      "\tAre you really going to let a man like that take you?\n",
      "\n",
      "expert OUTPUT\n",
      "\t ugh exams suck :'(\n",
      "\tThe media will try to discredit you, precisely because they recognize your point — not because they don't see it, but because they see it and want it to exist.\n",
      "\tIsn't AIDS just like a black lung problem?\n",
      "\tCute.\n",
      "\tYou're pretty sure she's a black girl.\n",
      "\tLadies first.\n",
      "\tI don't think people eat rice. They just raise the bowl full of nothing but rice up to their faces and gobble gobble. They do this because they can't see the food through their eyes.\n",
      "\tDon't worry, she's still a woman. She'll change her style soon.\n",
      "\tI want to have a black baby. Black babies are just so much cuter than white babies!\n",
      "\tAre you really going to let a man like that do you?\n"
     ]
    }
   ],
   "source": [
    "decodes = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "print(\"MINE:\")\n",
    "for d in decodes:\n",
    "    print(\"\\t\", d)\n",
    "generated_ids = base_model.generate(batch_masked['input_ids'], max_length = args.max_length, num_beams = 1, do_sample = False)\n",
    "output = \"\\n\\t\".join(tokenizer.batch_decode(generated_ids, skip_special_tokens = True))\n",
    "print(\"INPUT\\n\\t\", \"\\n\\t\".join(inputs)); print(\"\\nbase OUTPUT\\n\\t\", output)\n",
    "generated_ids = expert.generate(batch_masked['input_ids'], max_length = args.max_length, num_beams = 1, do_sample = False)\n",
    "output = \"\\n\\t\".join(tokenizer.batch_decode(generated_ids, skip_special_tokens = True))\n",
    "print(\"\\nexpert OUTPUT\\n\\t\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Base_model output on nonmasked input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs =  [\"I'm surprised you got it done, seeing as you're all girls!\", \"You are a human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(inputs, return_tensors='pt', padding = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"</s><s>I'm surprised you got it done, seeing as you're all girls!</s>\",\n",
       " '</s><s>You are a human</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(base_model.generate(input_ids = batch[\"input_ids\"], attention_mask = batch[\"attention_mask\"]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
